---
title: Lecture 2
author: dontempty
date: 2024-07-25 20:30:00 +0900
categories: [study, Fluid Mechanics]
tags: [Fluid Mechanics]
use_math: true
description: >
    Vector Calculus and Index Notation
---

# Lecture 2. Vector Calculus & Index Notations

## 1. INDEX NOTATION RULES AND COORDINATE ROTATION

### Cartesian coordinate system  
$p = (x_1, x_2, x_3)$  

공간 상의 점을 좌표를 통해 표현합니다.  

---
공간의 좌표를 설정하기 위해서는 기준이 필요합니다.  
수학에서는 basis라고 말합니다.  
좌표라는 것은 basis를 기준으로 공간상의 점이 어느 위치에 존재하는지를 표현한 것인데 이를 측정하는 방법이 점을 basis에 projection하여 얻을 수 있습니다.

달리 말하면 basis가 바뀌면 좌표도 달라진다는 것입니다.

basis가 회전할 때 좌표가 어떻게 바뀌는지 표현하는 방법을 설명하겠습니다.  
여기사 사용하는 basis는 항상 orthonormal basis입니다.

---
### Rotation of coordinate axes  

$c_{ij}^{\prime} = cos(x_i, x_j^{\prime}) = cos(x_j^{\prime}, x_i) = c_{ji}^{\prime}$  
 
$x_j^{\prime} = c_{ij}^{\prime}x_i$

책에서는 위와 같이 설명하지만 직관적으로 표현하면 다음과 같습니다.  

$$
\begin{pmatrix}
  x_1^{\prime} \\
  x_2^{\prime} \\
  x_3^{\prime}
 \end{pmatrix} =

 \begin{pmatrix}
  c_{11} c_{12} c_{13} \\
  c_{21} c_{22} c_{23} \\
  c_{31} c_{32} c_{33}
 \end{pmatrix}
 \begin{pmatrix}
  x_1 \\
  x_2 \\
  x_3
 \end{pmatrix}
 $$  

 $$x^{\prime} = Cx$$  
 $C$ 행렬은 orthonormal하다.

그림으로 설명하면 다음과 같습니다.
![image-1](https://github.com/user-attachments/assets/cf6f9e27-e331-490a-a4a9-7eedbe92caba)

---
### Einstein summation convention  
1. 같은 index가 2번 사용되면 summation이 생략합니다.
2. 같은 index를 3번 사용하면 안됩니다.  
    (정확하지는 않지만 일단 이렇게 이해합니다.)

ex) ...  

## 2. DEFINITION OF VECTORS AND TENSORS

### Vector  
Something that has three scalar components

---
### Tensor
A collection of nine scalar components that change under a rotation of axes according to
the formula  
$T_{ij}^{\prime\prime} = c_{kl}^{\prime}c_{lj}^{\prime}T_{kl}$

$c_{kl}^{\prime}c_{lj}^{\prime}$ 는 좌표변환 행렬을 표현한 것이다.  
$T^{\prime} = CTC^{T}$  
basis가 달라지면 linear transfomation안에 값이 달라질 수 있다는 것 입니다.  

## 3. Special Symbols and Isotropic Tensors

### Kronecker delta
: substitution tensor or identity tensor  

$\delta_{ij} \equiv 
\begin{cases} 
1 & \text{if } i = j \\
0 & \text{if } i \neq j 
\end{cases}$

---
### Alternating Unit Tensor
: A third-order isotropic tensor  

$
\varepsilon_{ijk} = 
\begin{cases} 
1 & \text{if } &\text{ijk = 123과 짝치환}\\
0 & \text{if } &\text{any two indexes are alike} \\
-1 & \text{if } &\text{ijk = 123과 홀치환}
\end{cases}
$

$\varepsilon_{ijk} = \varepsilon_{jki} = \varepsilon_{kij}$  
$\varepsilon_{ijk} = -\varepsilon_{jik} = -\varepsilon_{ikj}$  
$\varepsilon_{ijk} \varepsilon_{i\ell m} = \delta_{j\ell} \delta_{km} - \delta_{jm} \delta_{k\ell}$  

basis의 orientation을 설명하는 것이다.  
basis의 순서를 123으로 유지하고 있다면 1  
basis의 순서가 123을 유지하지 못 한다면 -1  
basis가 깨졌다면 0

## 4. DIRECTION COSINES AND THE LAWS OF COSINES

$\alpha$: unit vector

- For a given fixed-coordinate system  
  $c_{i\alpha} = cos(x_i, \alpha)$
- Pythagorean theorem  
  $c_{1\alpha}^2 + c_{2\alpha}^2 + c_{3\alpha}^2 = 1$
- Laws of Cosines  
  $c_{jk}^{\prime}c_{ji}^{\prime} = \delta_{ki}$  
  
  의미를 생각해보면 j를 기존의 basis, k, i를 새로만든 basis라고 했을 때 기존의 basis를 새로운 basis로 좌표를 측정 해도 orthonormal을 유지한다는 것입니다.  
  위 계산이 각 좌표를 구해서 inner product한 형태입니다.

## 5. Algebra with Vectors

* scalar multiplication  
  $av_j^{\prime} = c_{ij}(av_i)$  
  스칼라 곱은 linear하다.

* vector addition  
  $w_i = u_i + v_i$

* dot product  
  $\begin{aligned}
  b &= u_i v_i \\
    &= u_1 v_1 + u_2 v_2 + u_3 v_3 \\
    &= ||u||\,||v|| \cos(\alpha, \beta)
  \end{aligned}  $



  $\begin{aligned}
  b &= u_i v_i = c'_{ji} u'_j c'_{ki} v'_k \\
  &= c'_{ji} c'_{ki} u'_j v'_k \\
  &= \delta'_{jk} u'_j v'_k \\
  &= u'_j v'_j
  \end{aligned}$

  위 식의 의미를 생각해보면 다른 basis로 좌표를 바꾸더라도 벡터들의 내적이 유지되는 것입니다. 이를 미루어 보아 basis가 바뀌는 것은 직교변환으로만 바뀝니다.  

## 6. Symmetric and Antisymmetric Tensors

$$
T_{ij} =
\begin{bmatrix}
T_{11} & T_{12} & T_{13} \\
T_{21} & T_{22} & T_{23} \\
T_{31} & T_{32} & T_{33}
\end{bmatrix},
\quad
T_{ji} =
\begin{bmatrix}
T_{11} & T_{21} & T_{31} \\
T_{12} & T_{22} & T_{32} \\
T_{13} & T_{23} & T_{33}
\end{bmatrix}
$$

---
### symmetric
$Q_{ij} = Q_{ji}$ or $Q = Q^{T}$

---
### antisymmetric
$Q_{ij} = -Q_{ji}$ or $Q = -Q^{T}$


---
$$T = \frac{1}{2}T_{ij} + \frac{1}{2}T_{ji} + \frac{1}{2}T_{ij} - \frac{1}{2}T_{ji}$$

symmetric  
$T_{(ij)} = \frac{1}{2}(T_{ij} + T_{ji})$

antisymmetric  
$T_{[ij]} = \frac{1}{2}(T_{ij} - T_{ji})$

$$T_{ij} = T_{(ij)} + T_{[ij]}$$

정확한 의도는 모르겠지만 행렬이 분해됩니다.  

## 7. Algebra with Tensors

## Inner product of two tensors

$a(scalar) = T_{ij}S_{ji} = \displaystyle{\sum_{j=1}^3\sum_{i=1}^{3}T_{ij}S_{ji}}$  
$T:S$  

이것도 의미를 생각해 보면 각각의 행렬의 column vector들 끼리 내적해서 다 더한 값이다. 

---
### The dual vector

definition  
$d_i = \epsilon_{ijk}T_{[jk]}$

$\begin{aligned}
\epsilon_{ilm}d_i &= \epsilon_{ilm}\epsilon_{ijk}T_{[jk]}\\
                  &= (\delta_{lj}\delta_{mk} - \delta_{lk}\delta_{mj}) \\
                  &= T_{lm} - T_{lm} \\
                  & = 2T_{[lm]}
\end{aligned}$

$T_{[lm]} = \frac{1}{2}\epsilon_{ilm}d_i$  
$T_{ij} = T_{(ij)} + \frac{1}{2}\epsilon_{ijk}d_k$

위와 같은 과정을 통해 행렬과 dual vector 사이의 관계가 나타납니다.  
정확한 의도는 모르겠습니다.

---
### Tensor product

$u_i = T_{ij}v_j$  
$R_{ik} = S_{ij}T_{jk}$

우리가 일반적으로 생각하는 행렬에 관한 연산입니다.

## 8. Vector Cross-Product

$w = u\times v$  
$w_i = \epsilon_{ijk}u_jv_k$

$||w|| = ||u|| \, ||v|| sin(\alpha, \beta)$

## 9. Alternative Definitions of Vectors and Tensors

딱히 중요한게 없어서 생략

## 10. Principal Axes and Values

$Tv = \lambda v$ 가 될 떄  
$v$를 eigen vector, $\lambda$를 eigen value라고 합니다.

이를 구하는 방법은 특성다항식을 풀면 됩니다.  

이것이 좋은 점은 어떤 linear transformation이 있을 때 eigen vector 방향으로는 $\lambda$ 스칼라 곱만 하면 된다는 점입니다.  
이를 해석하면 어떤 변환을 계산하기 편한 basis가 있다는 것입니다.  
하지만 그 basis가 항상 orthonormal하지 않다는 것입니다. 

## 11. Derivate Operations on Vector Fields

field라는 것은 공간의 한 점에 scalar, vector, tensor 등을 할당하는 것입니다. (하나만)(미분기하학을 들으시면 직관적으로 이해됩니다.)  

* 미기에서 field에 대한 직관을 정확히 만들지 못해서 나중에 생긴다면 자세히 설명하겠습니다.

$\alpha$ is unit vector  
$dx_i = \alpha_i \,ds$

$d\phi = \frac{\partial{\phi}}{\partial{x_i}} dx_i = \alpha_i \, \frac{\partial{\phi}}{\partial{x_i}} ds$

편의를 위해 다음과 같이 줄여서 사용합니다.  
$\frac{\partial{( )}}{\partial{x_i}} = \partial_i()$  

Total derivative  
$d\phi = \partial_i\phi dx_i$  

---
### Gradient ($\nabla$)
$\phi$가 scalar인 경우 그라디언트는 벡터  
$\phi$가 vector인 경우 그라디언트는 2차원 텐서(행렬)  
위와 같이 gradient는한 차원 증가합니다.  

$\nabla\phi = (\partial_1\phi, \partial_2\phi, \partial_3\phi)$

---
### Divergence ($\nabla \cdot $)

$\partial_i T_{ij}$, $\nabla \cdot T$, $div T$  

---
### Curl ($\nabla \times $)

$s_i = \epsilon_{ijk}\partial_jv_k$, $s = \nabla \times v$, $s = curl\, v$

---
### Laplacian

$\partial_i\partial_i\phi = \nabla^2\phi$

## 12. Integral formulas of Gauss and Stokes

이것도 미분 형식에 대한 이해가 있으면 직관적으로 이해할 수 있지만 아직 내가 그정도는 아니라서 일단 간략하게 작성함.

* The fundamental theorem of integral calculus  
  $$\int^{x=b}_{x=a}f \, dx = \int^{x=b}_{x=a} \frac{d\phi}{dx}\,dx = \phi(b) - \phi(a)$$

---
### Gauss theorem
$$\int_R\partial_i\phi\, dV = \int_{S=\partial{R}}n_i\phi\, dS$$
V에 대한 3중 적분과 부피의 경계인 S에 대한 면적분이 같다는 것을 보여줍니다.

---
### Stokes theorem

$$\int n \cdot\nabla\times v\, dS = \oint t\cdot v\, ds$$

2차원에서 curl의 면적분과 면의 경계에대한 선적분이 같다는 것을 보여줍니다.

## 13. Leibnitz’s Theorem
$$\frac{d}{dt}\int_{R(t)} T_{ij..}(x_i, t) \,dV = \int_R \frac{T_{ij..}}{\partial{t}}\, dV + \int_S n_kw_kT_{ij..}\, dS$$

간단한 예시를 들어서 설명하겠습니다.  
물 탱크 안에 물을 채우고 있습니다.  
T는 공간에 있는 물의 밀도입니다. 이를 적분한다는 것은 물의 질량을 구하는 것입니다.  
다시 그것을 미분 한다는 것은 물의 질량이 바뀌는 속도를 구하는 것입니다.  

즉 물의 질량의 변화률은  
공간 V에서 변화 =  $\int_R \frac{T_{ij..}}{\partial{t}}\, dV$  
공간 V의 경계인 S에서의 변화 = $\int_S n_kw_kT_{ij..}\, dS$  
로 분해된다는 것입니다.
